# Global variables
n = 29
nodes = 100
nodes2 = 150
device = torch.device('cpu')
# cuda will only create a significant speedup for large/deep networks and batched training
#device = torch.device('cuda') 

#randomly initialized weights, with zeros for the biases, for the actor
theta = 0.05*torch.ones((1,nodes2), device = device, dtype=torch.float)

alpha_w = 0.1 # step sizes using for the neural network (first layer)
alpha_th = 0.001 # (second layer)
lam_w = 0.7 # lambda parameter in TD(lam-bda)
lam_th = 0.7

competition_games = 10
training_steps = 10
Load_data = True
loadtrainstep = 10

# randomly initialized weights, with zeros for the biases, for the critic
if Load_data == True:
    w1 = torch.load('./Data/w1_trained_'+str(loadtrainstep)+'.pth')
    w2 = torch.load('./Data/w2_trained_'+str(loadtrainstep)+'.pth')
    w3 = torch.load('./Data/w3_trained_'+str(loadtrainstep)+'.pth')
    b1 = torch.load('./Data/b1_trained_'+str(loadtrainstep)+'.pth')
    b2 = torch.load('./Data/b2_trained_'+str(loadtrainstep)+'.pth')
    b3 = torch.load('./Data/b3_trained_'+str(loadtrainstep)+'.pth')
    wins_against_random = np.load('./Data/wins_against_random.npy')
    #wins_against_random = np.concatenate(wins_against_random, np.zeros(training_steps), 0)
else:
    loadtrainstep = 0  
    w1 = Variable(torch.randn(nodes,7*(n-1)*2, device = device, dtype=torch.float), requires_grad = True)
    b1 = Variable(torch.zeros(nodes,1, device = device, dtype=torch.float), requires_grad = True)
    w2 = Variable(torch.randn(nodes2,nodes, device = device, dtype=torch.float), requires_grad = True)
    b2 = Variable(torch.zeros((nodes2,1), device = device, dtype=torch.float), requires_grad = True)
    w3 = Variable(torch.randn(1,nodes2, device = device, dtype=torch.float), requires_grad = True)
    b3 = Variable(torch.zeros((1,1), device = device, dtype=torch.float), requires_grad = True)

# compete for "competition_games" vs a random player, 
# then train for "training_steps" using self-play.
for i in range(0,100):
    start = time.time()
    wins_for_player_1 = 0
    loss_for_player_1 = 0
    for j in range(competition_games):
        winner = play_a_game_random(commentary = False)
        if (winner == 1):
            wins_for_player_1 += 1.0
        else:
            loss_for_player_1 += 1.0

    end = time.time()
    print(end - start)
    print(wins_for_player_1, loss_for_player_1)
    
    start = time.time()
    learnit(training_steps, lam_w,lam_th, alpha_w, alpha_th)
    end = time.time()
    print(end - start)
    
    # Save the NN variables
    torch.save(w1, './Data/w1_trained_'+str((i+1)*training_steps)+'.pth')
    torch.save(w2, './Data/w2_trained_'+str((i+1)*training_steps)+'.pth')
    torch.save(w3, './Data/w3_trained_'+str((i+1)*training_steps)+'.pth')
    torch.save(b1, './Data/b1_trained_'+str((i+1)*training_steps)+'.pth')
    torch.save(b2, './Data/b2_trained_'+str((i+1)*training_steps)+'.pth')
    torch.save(b3, './Data/b3_trained_'+str((i+1)*training_steps)+'.pth')
    np.save('./Data/wins_against_random.npy', wins_for_player_1)
    

'''
Parametrar:
0.05*torch
alpha_w = 0.05 # step sizes using for the neural network (first layer)
alpha_th = 0.01 # (second layer)
lam_w = 0.7 # lambda parameter in TD(lam-bda)
lam_th = 0.7
Virkaði vel (Öfug reward.)
Sama og fyrir ofan með öfug reward nema 0 í staðin fyrir -1
Varð skelfilega lélegur.
Reward rétt nema með 1 og 0.
'''
